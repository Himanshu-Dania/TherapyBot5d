{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-02-15T09:12:07.968462Z",
     "iopub.status.busy": "2025-02-15T09:12:07.968056Z",
     "iopub.status.idle": "2025-02-15T09:12:09.237035Z",
     "shell.execute_reply": "2025-02-15T09:12:09.235827Z",
     "shell.execute_reply.started": "2025-02-15T09:12:07.968430Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# Yvou can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-15T09:12:09.238536Z",
     "iopub.status.busy": "2025-02-15T09:12:09.238049Z",
     "iopub.status.idle": "2025-02-15T09:12:19.014578Z",
     "shell.execute_reply": "2025-02-15T09:12:19.013332Z",
     "shell.execute_reply.started": "2025-02-15T09:12:09.238505Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.47.0)\n",
      "Requirement already satisfied: peft in /usr/local/lib/python3.10/dist-packages (0.14.0)\n",
      "Collecting bitsandbytes\n",
      "  Downloading bitsandbytes-0.45.2-py3-none-manylinux_2_24_x86_64.whl.metadata (5.8 kB)\n",
      "Requirement already satisfied: accelerate in /usr/local/lib/python3.10/dist-packages (1.2.1)\n",
      "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (3.2.0)\n",
      "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.5.1+cu121)\n",
      "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (0.2.0)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.17.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.24.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.28.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.21.0)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from peft) (5.9.5)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (19.0.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.2.3)\n",
      "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2024.9.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets) (2024.9.0)\n",
      "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.11.11)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (2.4.4)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.2)\n",
      "Requirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (5.0.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (25.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.1.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (0.2.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.18.3)\n",
      "Requirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->transformers) (1.3.8)\n",
      "Requirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->transformers) (1.2.4)\n",
      "Requirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->transformers) (0.1.1)\n",
      "Requirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->transformers) (2025.0.1)\n",
      "Requirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->transformers) (2022.0.0)\n",
      "Requirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->transformers) (2.4.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2025.1.31)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (3.0.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2025.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2025.1)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
      "Requirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.17->transformers) (2024.2.0)\n",
      "Requirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.17->transformers) (2022.0.0)\n",
      "Requirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->transformers) (1.2.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy>=1.17->transformers) (2024.2.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy>=1.17->transformers) (2024.2.0)\n",
      "Downloading bitsandbytes-0.45.2-py3-none-manylinux_2_24_x86_64.whl (69.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m69.7/69.7 MB\u001b[0m \u001b[31m18.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0mm\n",
      "\u001b[?25hInstalling collected packages: bitsandbytes\n",
      "Successfully installed bitsandbytes-0.45.2\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers peft bitsandbytes accelerate datasets torch sentencepiece\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-15T09:12:20.228895Z",
     "iopub.status.busy": "2025-02-15T09:12:20.228529Z",
     "iopub.status.idle": "2025-02-15T09:12:20.237683Z",
     "shell.execute_reply": "2025-02-15T09:12:20.236605Z",
     "shell.execute_reply.started": "2025-02-15T09:12:20.228866Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "token=os.getenv(\"HF_TOKEN\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-15T09:12:19.016120Z",
     "iopub.status.busy": "2025-02-15T09:12:19.015731Z",
     "iopub.status.idle": "2025-02-15T09:12:20.101236Z",
     "shell.execute_reply": "2025-02-15T09:12:20.100100Z",
     "shell.execute_reply.started": "2025-02-15T09:12:19.016089Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "token=os.getenv(\"HF_TOKEN\")\n",
    "login(token)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-15T09:12:20.103094Z",
     "iopub.status.busy": "2025-02-15T09:12:20.102560Z",
     "iopub.status.idle": "2025-02-15T09:12:20.227260Z",
     "shell.execute_reply": "2025-02-15T09:12:20.225937Z",
     "shell.execute_reply.started": "2025-02-15T09:12:20.103050Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-15T09:12:20.259363Z",
     "iopub.status.busy": "2025-02-15T09:12:20.259003Z",
     "iopub.status.idle": "2025-02-15T09:12:27.933540Z",
     "shell.execute_reply": "2025-02-15T09:12:27.932433Z",
     "shell.execute_reply.started": "2025-02-15T09:12:20.259335Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6a0651e717344e7aa22d456402260d87",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/510 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a7ca7a77a2cb40b2aaf5a8b564a4efe8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train.txt:   0%|          | 0.00/4.04M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "903c5ef6cb5f445e8718677d16c7fc32",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "valid.txt:   0%|          | 0.00/865k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cebeced930e44d3997cfe359a69c9aff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "test.txt:   0%|          | 0.00/881k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "20c54c66bc7f400785a152a26c630ac8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/910 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dca835bc55564edd86788711950b8437",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split:   0%|          | 0/195 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dc6f09f718584792aeebd3914c7f8b05",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/195 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "ds = load_dataset(\"thu-coai/esconv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-15T09:12:27.935039Z",
     "iopub.status.busy": "2025-02-15T09:12:27.934563Z",
     "iopub.status.idle": "2025-02-15T09:12:27.945903Z",
     "shell.execute_reply": "2025-02-15T09:12:27.944719Z",
     "shell.execute_reply.started": "2025-02-15T09:12:27.935009Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "train_data=ds[\"train\"][\"text\"]\n",
    "test_data = (ds[\"test\"][\"text\"])\n",
    "validation_data = (ds[\"validation\"][\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-15T09:12:27.947659Z",
     "iopub.status.busy": "2025-02-15T09:12:27.947246Z",
     "iopub.status.idle": "2025-02-15T09:12:27.967973Z",
     "shell.execute_reply": "2025-02-15T09:12:27.966756Z",
     "shell.execute_reply.started": "2025-02-15T09:12:27.947616Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "# Suppose your raw data is stored in the variable `data`\n",
    "# where each element is a JSON string (one conversation per line).\n",
    "# For example:\n",
    "# data = [\n",
    "#    '{\"experience_type\": \"Current Experience\", \"emotion_type\": \"anxiety\", ... }',\n",
    "#    '{\"experience_type\": \"...\", ... }',\n",
    "#    ...\n",
    "# ]\n",
    "def create_dataset(data, save_path):\n",
    "    training_samples = []  # To store the transformed examples\n",
    "    \n",
    "    for line in data:\n",
    "        # Parse the JSON for one conversation\n",
    "        conv = json.loads(line)\n",
    "        dialog = conv[\"dialog\"]\n",
    "    \n",
    "        # First, merge consecutive utterances by the same speaker.\n",
    "        merged_blocks = []  # Each block is a dict: {speaker, text, [strategies]}\n",
    "        for utt in dialog:\n",
    "            speaker = utt[\"speaker\"].strip()\n",
    "            text = utt[\"text\"].strip()\n",
    "            strategy = utt.get(\"strategy\")  # might be None for user turns\n",
    "    \n",
    "            if merged_blocks and merged_blocks[-1][\"speaker\"] == speaker:\n",
    "                # Same speaker as previous block → merge texts.\n",
    "                merged_blocks[-1][\"text\"] += \" \" + text\n",
    "                # For system turns, if a strategy is present, add it to the list.\n",
    "                if speaker == \"sys\" and strategy:\n",
    "                    merged_blocks[-1][\"strategies\"].append(strategy)\n",
    "            else:\n",
    "                # Start a new block.\n",
    "                if speaker == \"sys\":\n",
    "                    # For sys blocks, store the strategy in a list.\n",
    "                    merged_blocks.append({\n",
    "                        \"speaker\": speaker,\n",
    "                        \"text\": text,\n",
    "                        \"strategies\": [strategy] if strategy else []\n",
    "                    })\n",
    "                else:\n",
    "                    merged_blocks.append({\n",
    "                        \"speaker\": speaker,\n",
    "                        \"text\": text\n",
    "                    })\n",
    "    \n",
    "        # Next, generate training samples.\n",
    "        # The idea is: for each system (sys) block (which carries a strategy),\n",
    "        # create a sample whose input text is the conversation history _before_ that sys block.\n",
    "        for i, block in enumerate(merged_blocks):\n",
    "            if block[\"speaker\"] == \"sys\" and block.get(\"strategies\"):\n",
    "                # Build the conversation context from all previous blocks.\n",
    "                context_parts = []\n",
    "                for b in merged_blocks[:i]:\n",
    "                    if b[\"speaker\"] == \"usr\":\n",
    "                        context_parts.append(f\"usr: {b['text']}\")\n",
    "                    elif b[\"speaker\"] == \"sys\":\n",
    "                        # Join the strategies with a comma if there are multiple.\n",
    "                        strat = \", \".join(b.get(\"strategies\", []))\n",
    "                        context_parts.append(f\"sys({strat}): {b['text']}\")\n",
    "                context = \" \".join(context_parts).strip()\n",
    "    \n",
    "                # The label is the strategy (or list of strategies) from the current sys block.\n",
    "                training_samples.append({\n",
    "                    \"text\": context,\n",
    "                    \"label\": block[\"strategies\"]\n",
    "                })\n",
    "    \n",
    "    # Optionally, save the transformed data as a CSV (each row: text, label)\n",
    "    df = pd.DataFrame(training_samples)\n",
    "    df.to_csv(save_path, index=False)\n",
    "    \n",
    "    # For inspection, print the first few samples:\n",
    "    for sample in training_samples[:2]:\n",
    "        print(\"Text:\", sample[\"text\"])\n",
    "        print(\"Label:\", sample[\"label\"])\n",
    "        print(\"------\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-15T09:12:27.969485Z",
     "iopub.status.busy": "2025-02-15T09:12:27.969115Z",
     "iopub.status.idle": "2025-02-15T09:12:27.998070Z",
     "shell.execute_reply": "2025-02-15T09:12:27.996735Z",
     "shell.execute_reply.started": "2025-02-15T09:12:27.969447Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "def create_dataset(data, save_path):\n",
    "    training_samples = []  # To store the transformed examples\n",
    "    \n",
    "    for line in data:\n",
    "        # Parse the JSON for one conversation\n",
    "        conv = json.loads(line)\n",
    "        dialog = conv[\"dialog\"]\n",
    "    \n",
    "        # First, merge consecutive utterances by the same speaker.\n",
    "        merged_blocks = []  # Each block is a dict: {speaker, text, [strategies]}\n",
    "        for utt in dialog:\n",
    "            speaker = utt[\"speaker\"].strip()\n",
    "            text = utt[\"text\"].strip()\n",
    "            strategy = utt.get(\"strategy\")  # might be None for user turns\n",
    "    \n",
    "            if merged_blocks and merged_blocks[-1][\"speaker\"] == speaker:\n",
    "                # Same speaker as previous block → merge texts.\n",
    "                merged_blocks[-1][\"text\"] += \" \" + text\n",
    "                # For system turns, if a strategy is present, add it to the list.\n",
    "                if speaker == \"sys\" and strategy:\n",
    "                    merged_blocks[-1][\"strategies\"].append(strategy)\n",
    "            else:\n",
    "                # Start a new block.\n",
    "                if speaker == \"sys\":\n",
    "                    # For sys blocks, store the strategy in a list.\n",
    "                    merged_blocks.append({\n",
    "                        \"speaker\": speaker,\n",
    "                        \"text\": text,\n",
    "                        \"strategies\": [strategy] if strategy else []\n",
    "                    })\n",
    "                else:\n",
    "                    merged_blocks.append({\n",
    "                        \"speaker\": speaker,\n",
    "                        \"text\": text\n",
    "                    })\n",
    "    \n",
    "        # Next, generate training samples.\n",
    "        for i, block in enumerate(merged_blocks):\n",
    "            if block[\"speaker\"] == \"sys\" and block.get(\"strategies\"):\n",
    "                # Build the conversation context from the previous utterances (max 6).\n",
    "                context_parts = []\n",
    "                start_idx = max(0, i - 4)  # Keep only the last 6 utterances\n",
    "                \n",
    "                for b in merged_blocks[start_idx:i]:\n",
    "                    if b[\"speaker\"] == \"usr\":\n",
    "                        context_parts.append(f\"usr: {b['text']}\")\n",
    "                    elif b[\"speaker\"] == \"sys\":\n",
    "                        strat = \", \".join(b.get(\"strategies\", []))\n",
    "                        context_parts.append(f\"sys({strat}): {b['text']}\")\n",
    "                \n",
    "                context = \" \".join(context_parts).strip()\n",
    "                if context is not None:\n",
    "                    training_samples.append({\n",
    "                        \"text\": context,\n",
    "                        \"label\": block[\"strategies\"]\n",
    "                    })\n",
    "    \n",
    "    # Save the transformed data as a CSV\n",
    "    df = pd.DataFrame(training_samples)\n",
    "    df.to_csv(save_path, index=False)\n",
    "    \n",
    "    # Print first few samples for inspection\n",
    "    for sample in training_samples[:2]:\n",
    "        print(\"Text:\", sample[\"text\"])\n",
    "        print(\"Label:\", sample[\"label\"])\n",
    "        print(\"------\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-15T09:12:27.999619Z",
     "iopub.status.busy": "2025-02-15T09:12:27.999323Z",
     "iopub.status.idle": "2025-02-15T09:12:28.476144Z",
     "shell.execute_reply": "2025-02-15T09:12:28.474815Z",
     "shell.execute_reply.started": "2025-02-15T09:12:27.999593Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: usr: Hello good afternoon.\n",
      "Label: ['Question']\n",
      "------\n",
      "Text: usr: Hello good afternoon. sys(Question): Hi, good afternoon. usr: I'm feeling anxious that I am going to lose my job.\n",
      "Label: ['Reflection of feelings']\n",
      "------\n",
      "Text: \n",
      "Label: ['Others']\n",
      "------\n",
      "Text: sys(Others): Hello. How are you today? usr: hi i am okay, a little bit sad though\n",
      "Label: ['Question']\n",
      "------\n",
      "Text: \n",
      "Label: ['Question']\n",
      "------\n",
      "Text: sys(Question): Hi, what I can help you with today? usr: Hi I had problems in the past and somehow right now\n",
      "Label: ['Restatement or Paraphrasing']\n",
      "------\n"
     ]
    }
   ],
   "source": [
    "create_dataset(train_data, \"train.csv\")\n",
    "create_dataset(test_data, \"test.csv\")\n",
    "create_dataset(validation_data, \"val.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-15T09:12:28.478238Z",
     "iopub.status.busy": "2025-02-15T09:12:28.477699Z",
     "iopub.status.idle": "2025-02-15T09:12:28.483021Z",
     "shell.execute_reply": "2025-02-15T09:12:28.481596Z",
     "shell.execute_reply.started": "2025-02-15T09:12:28.478166Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# from datasets import load_dataset\n",
    "\n",
    "# # Example: Load a CSV dataset with \"text\" and \"label\" columns\n",
    "# dataset = load_dataset(\"csv\", data_files={\"train\": \"train.csv\", \"test\": \"test.csv\", \"val\": \"val.csv\"})\n",
    "# dataset = dataset.filter(lambda example: example[\"text\"] is not None)\n",
    "# print(dataset[\"train\"][10])  # Check a sample\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-15T09:12:28.484702Z",
     "iopub.status.busy": "2025-02-15T09:12:28.484327Z",
     "iopub.status.idle": "2025-02-15T09:12:28.503622Z",
     "shell.execute_reply": "2025-02-15T09:12:28.502466Z",
     "shell.execute_reply.started": "2025-02-15T09:12:28.484674Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "\n",
    "# # Load the dataset\n",
    "# df = pd.read_csv(\"val.csv\")\n",
    "\n",
    "# # Compute the max number of words in the \"text\" column\n",
    "# max_words = df[\"text\"].apply(lambda x: len(str(x).split())).max()\n",
    "\n",
    "# print(\"Maximum number of words in 'text':\", max_words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-15T09:12:28.505338Z",
     "iopub.status.busy": "2025-02-15T09:12:28.504918Z",
     "iopub.status.idle": "2025-02-15T09:12:28.523917Z",
     "shell.execute_reply": "2025-02-15T09:12:28.522780Z",
     "shell.execute_reply.started": "2025-02-15T09:12:28.505302Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "# import torch\n",
    "\n",
    "# model_name = \"meta-llama/Meta-Llama-3-8B\"  # Or any other available LLaMA 3 model\n",
    "# # model_name = \"mistralai/Mistral-7B-Instruct-v0.3\"\n",
    "# tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "# model = AutoModelForCausalLM.from_pretrained(\n",
    "#     model_name,\n",
    "#     device_map=\"auto\",\n",
    "#     torch_dtype=torch.float16,\n",
    "#     use_auth_token=True\n",
    "# )\n",
    "# tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-15T09:12:28.525520Z",
     "iopub.status.busy": "2025-02-15T09:12:28.525064Z",
     "iopub.status.idle": "2025-02-15T09:12:28.545724Z",
     "shell.execute_reply": "2025-02-15T09:12:28.544691Z",
     "shell.execute_reply.started": "2025-02-15T09:12:28.525409Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# from peft import LoraConfig, get_peft_model\n",
    "\n",
    "# lora_config = LoraConfig(\n",
    "#     r=8,                    # Rank of LoRA matrices\n",
    "#     lora_alpha=32,          # Scaling factor\n",
    "#     target_modules=[\"q_proj\", \"v_proj\"],  # Apply LoRA to attention layers\n",
    "#     lora_dropout=0.1,\n",
    "#     bias=\"none\",\n",
    "#     task_type=\"CAUSAL_LM\",  # Fine-tuning for text generation/classification\n",
    "# )\n",
    "\n",
    "# model = get_peft_model(model, lora_config)\n",
    "# model.print_trainable_parameters()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-15T09:12:28.547073Z",
     "iopub.status.busy": "2025-02-15T09:12:28.546739Z",
     "iopub.status.idle": "2025-02-15T09:12:28.563430Z",
     "shell.execute_reply": "2025-02-15T09:12:28.562129Z",
     "shell.execute_reply.started": "2025-02-15T09:12:28.547045Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-15T09:12:28.565111Z",
     "iopub.status.busy": "2025-02-15T09:12:28.564773Z",
     "iopub.status.idle": "2025-02-15T09:12:28.583409Z",
     "shell.execute_reply": "2025-02-15T09:12:28.582324Z",
     "shell.execute_reply.started": "2025-02-15T09:12:28.565084Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# def tokenize_function(example):\n",
    "#     # print (example)\n",
    "#     return tokenizer(\n",
    "#         example[\"text\"],\n",
    "#         padding=\"max_length\",\n",
    "#         truncation=True,\n",
    "#         max_length=1024\n",
    "#     )\n",
    "\n",
    "# tokenized_datasets = dataset.map(tokenize_function, batched=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-15T09:12:28.585306Z",
     "iopub.status.busy": "2025-02-15T09:12:28.584773Z",
     "iopub.status.idle": "2025-02-15T09:12:28.602967Z",
     "shell.execute_reply": "2025-02-15T09:12:28.601717Z",
     "shell.execute_reply.started": "2025-02-15T09:12:28.585264Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# from transformers import TrainingArguments, Trainer\n",
    "\n",
    "# training_args = TrainingArguments(\n",
    "#     output_dir=\"./llama3_lora_text_classification\",\n",
    "#     evaluation_strategy=\"epoch\",\n",
    "#     save_strategy=\"epoch\",\n",
    "#     per_device_train_batch_size=4,\n",
    "#     per_device_eval_batch_size=4,\n",
    "#     num_train_epochs=3,\n",
    "#     logging_steps=1,  # Log more frequently\n",
    "#     learning_rate=2e-4,\n",
    "#     fp16=True,  # Use mixed precision training\n",
    "#     save_total_limit=2,\n",
    "#     push_to_hub=False,\n",
    "#     report_to=\"none\",  # Disable WandB if not needed\n",
    "#     logging_dir=\"./logs\",  # Specify log directory\n",
    "#     disable_tqdm=False,  # Ensure tqdm progress bars are visible\n",
    "# )\n",
    "\n",
    "# trainer = Trainer(\n",
    "#     model=model,\n",
    "#     args=training_args,\n",
    "#     train_dataset=tokenized_datasets[\"train\"],\n",
    "#     eval_dataset=tokenized_datasets[\"test\"],\n",
    "# )\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-15T09:12:28.604475Z",
     "iopub.status.busy": "2025-02-15T09:12:28.604114Z",
     "iopub.status.idle": "2025-02-15T09:12:28.627093Z",
     "shell.execute_reply": "2025-02-15T09:12:28.625994Z",
     "shell.execute_reply.started": "2025-02-15T09:12:28.604448Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# trainer.train()  # Start training with visible logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-15T09:12:28.632692Z",
     "iopub.status.busy": "2025-02-15T09:12:28.632343Z",
     "iopub.status.idle": "2025-02-15T09:12:28.646607Z",
     "shell.execute_reply": "2025-02-15T09:12:28.645354Z",
     "shell.execute_reply.started": "2025-02-15T09:12:28.632666Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# model.save_pretrained(\"mistral_strategy_classification\")\n",
    "# tokenizer.save_pretrained(\"mistral_strategy_classification\")\n",
    "\n",
    "# # Test\n",
    "# text = \"sys(Question): Hello! How are you tonight? usr: hanging in there, kinda freaking out a little sys(Affirmation and Reassurance, Question): I'm sorry to hear you are freaking out. Perhaps a friendly listening ear can encourage you? Why are you freaking out? usr: I think that'll help, yeah sys(Restatement or Paraphrasing): Sometimes just a friend to hear you out and encourage you can really make a difference. Life has its ups and down. usr: I decided to leave my husband, he keeps snapping at me and won't listen to me or let me do anything\"\n",
    "# inputs = tokenizer(text, return_tensors=\"pt\").to(\"cuda\")\n",
    "# outputs = model.generate(**inputs)\n",
    "# print(outputs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-15T09:12:28.649487Z",
     "iopub.status.busy": "2025-02-15T09:12:28.649018Z",
     "iopub.status.idle": "2025-02-15T09:12:28.664531Z",
     "shell.execute_reply": "2025-02-15T09:12:28.663517Z",
     "shell.execute_reply.started": "2025-02-15T09:12:28.649449Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# import ast\n",
    "# from datasets import load_dataset\n",
    "# from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer, DataCollatorWithPadding\n",
    "# import torch\n",
    "\n",
    "# # ============================\n",
    "# # 1. Define the Label Set\n",
    "# # ============================\n",
    "\n",
    "# # List all possible strategies (adjust if needed)\n",
    "# strategy_labels = [\n",
    "#     \"Question\",\n",
    "#     \"Restatement or Paraphrasing\",\n",
    "#     \"Reflection of feelings\",\n",
    "#     \"Self-disclosure\",\n",
    "#     \"Affirmation and Reassurance\",\n",
    "#     \"Providing Suggestions\",\n",
    "#     \"Information\",\n",
    "#     \"Others\"\n",
    "# ]\n",
    "# num_labels = len(strategy_labels)\n",
    "# # Create mapping dictionaries to convert between labels and indices.\n",
    "# label2id = {label: i for i, label in enumerate(strategy_labels)}\n",
    "# id2label = {i: label for i, label in enumerate(strategy_labels)}\n",
    "\n",
    "# # ============================\n",
    "# # 2. Load Your CSV Datasets\n",
    "# # ============================\n",
    "\n",
    "# # Assumes you have train.csv, val.csv, and test.csv in your working directory.\n",
    "# data_files = {\n",
    "#     \"train\": \"train.csv\",\n",
    "#     \"validation\": \"val.csv\",\n",
    "#     \"test\": \"test.csv\"\n",
    "# }\n",
    "# raw_datasets = load_dataset(\"csv\", data_files=data_files)\n",
    "\n",
    "# # ============================\n",
    "# # 3. Preprocessing Function\n",
    "# # ============================\n",
    "\n",
    "# def preprocess_function(examples):\n",
    "#     # Tokenize the input text using the LLaMA tokenizer.\n",
    "#     # Here we use a max_length of 512; adjust as needed.\n",
    "#     encoding = tokenizer(examples[\"text\"], truncation=True, padding=\"max_length\", max_length=1024)\n",
    "    \n",
    "#     # Convert the label string into a multi-hot vector.\n",
    "#     multi_hot_labels = []\n",
    "#     for label_str in examples[\"label\"]:\n",
    "#         # Convert string to list. For example, the string \"['Question']\" becomes a list.\n",
    "#         try:\n",
    "#             labels = ast.literal_eval(label_str)\n",
    "#         except Exception:\n",
    "#             # If conversion fails, treat it as a single label (or empty).\n",
    "#             labels = [label_str] if label_str else [\"Others\"]\n",
    "        \n",
    "#         # Initialize a vector of zeros for each possible strategy.\n",
    "#         multi_hot = [0] * num_labels\n",
    "#         for lab in labels:\n",
    "#             if lab in label2id:\n",
    "#                 multi_hot[label2id[lab]] = 1\n",
    "#         multi_hot_labels.append(multi_hot)\n",
    "    \n",
    "#     # Add the processed labels into the encoding.\n",
    "#     encoding[\"labels\"] = multi_hot_labels\n",
    "#     return encoding\n",
    "# # "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-15T09:12:28.665901Z",
     "iopub.status.busy": "2025-02-15T09:12:28.665532Z",
     "iopub.status.idle": "2025-02-15T09:12:28.686621Z",
     "shell.execute_reply": "2025-02-15T09:12:28.685542Z",
     "shell.execute_reply.started": "2025-02-15T09:12:28.665858Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# raw_datasets[\"train\"][7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-15T09:12:28.688237Z",
     "iopub.status.busy": "2025-02-15T09:12:28.687759Z",
     "iopub.status.idle": "2025-02-15T09:12:28.708460Z",
     "shell.execute_reply": "2025-02-15T09:12:28.707279Z",
     "shell.execute_reply.started": "2025-02-15T09:12:28.688171Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# # ============================\n",
    "# # 4. Load the Tokenizer & Model\n",
    "# # ============================\n",
    "\n",
    "# # Replace with your LLaMA 7B model identifier.\n",
    "# model_name = \"MaziyarPanahi/Mistral-7B-Instruct-v0.3-GGUF\"\n",
    "\n",
    "# # Load the tokenizer.\n",
    "# tokenizer = AutoTokenizer.from_pretrained(model_name, use_auth_token=True)\n",
    "\n",
    "# # Load the model with a classification head.\n",
    "# # Setting problem_type to \"multi_label_classification\" configures the loss as BCEWithLogitsLoss.\n",
    "# model = AutoModelForSequenceClassification.from_pretrained(\n",
    "#     model_name,\n",
    "#     num_labels=num_labels,\n",
    "#     problem_type=\"multi_label_classification\",\n",
    "#     use_auth_token=True\n",
    "# )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-15T09:12:28.710123Z",
     "iopub.status.busy": "2025-02-15T09:12:28.709726Z",
     "iopub.status.idle": "2025-02-15T09:12:28.726168Z",
     "shell.execute_reply": "2025-02-15T09:12:28.725163Z",
     "shell.execute_reply.started": "2025-02-15T09:12:28.710093Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-15T09:12:28.727605Z",
     "iopub.status.busy": "2025-02-15T09:12:28.727272Z",
     "iopub.status.idle": "2025-02-15T09:12:28.744467Z",
     "shell.execute_reply": "2025-02-15T09:12:28.743253Z",
     "shell.execute_reply.started": "2025-02-15T09:12:28.727580Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# # ============================\n",
    "# # 5. Tokenize the Datasets\n",
    "# # ============================\n",
    "\n",
    "# # Map the preprocessing function over the entire dataset.\n",
    "# # remove_columns removes the original columns (e.g., text and label strings) after processing.\n",
    "# raw_datasets = raw_datasets.filter(lambda example: example[\"text\"] is not None)\n",
    "# tokenized_datasets = raw_datasets.map(\n",
    "#     preprocess_function,\n",
    "#     batched=False,\n",
    "#     remove_columns=raw_datasets[\"train\"].column_names\n",
    "# )\n",
    "\n",
    "# # Create a data collator to handle dynamic padding.\n",
    "# data_collator = DataCollatorWithPadding(tokenizer)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-15T09:12:28.745968Z",
     "iopub.status.busy": "2025-02-15T09:12:28.745603Z",
     "iopub.status.idle": "2025-02-15T09:12:28.761561Z",
     "shell.execute_reply": "2025-02-15T09:12:28.760492Z",
     "shell.execute_reply.started": "2025-02-15T09:12:28.745936Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# del raw_datasets\n",
    "# del ds\n",
    "# del dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-15T09:12:28.763088Z",
     "iopub.status.busy": "2025-02-15T09:12:28.762740Z",
     "iopub.status.idle": "2025-02-15T09:12:28.781900Z",
     "shell.execute_reply": "2025-02-15T09:12:28.780325Z",
     "shell.execute_reply.started": "2025-02-15T09:12:28.763044Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# tokenized_datasets[\"train\"][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-15T09:12:28.783587Z",
     "iopub.status.busy": "2025-02-15T09:12:28.783135Z",
     "iopub.status.idle": "2025-02-15T09:12:28.800767Z",
     "shell.execute_reply": "2025-02-15T09:12:28.799687Z",
     "shell.execute_reply.started": "2025-02-15T09:12:28.783546Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# # ============================\n",
    "# # 6. Set Up TrainingArguments & Trainer\n",
    "# # ============================\n",
    "\n",
    "# training_args = TrainingArguments(\n",
    "#     output_dir=\"./llama_multi_label\",\n",
    "#     evaluation_strategy=\"epoch\",\n",
    "#     save_strategy=\"epoch\",\n",
    "#     learning_rate=2e-5,\n",
    "#     per_device_train_batch_size=4,\n",
    "#     per_device_eval_batch_size=4,\n",
    "#     num_train_epochs=3,\n",
    "#     weight_decay=0.01,\n",
    "#     # fp16=True,  # Use mixed precision (if supported)\n",
    "#     save_total_limit=2,\n",
    "#     load_best_model_at_end=True,\n",
    "#     report_to=\"none\",  # Disable WandB if not needed\n",
    "#     logging_dir=\"./logs\",  # Specify log directory\n",
    "#     disable_tqdm=False,  # Ensure tqdm progress bars are visible\n",
    "#     logging_steps=1,  # Log more frequently\n",
    "# )\n",
    "\n",
    "# trainer = Trainer(\n",
    "#     model=model,\n",
    "#     args=training_args,\n",
    "#     train_dataset=tokenized_datasets[\"train\"],\n",
    "#     eval_dataset=tokenized_datasets[\"validation\"],\n",
    "#     tokenizer=tokenizer,\n",
    "#     data_collator=data_collator,\n",
    "# )\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-15T09:12:28.802330Z",
     "iopub.status.busy": "2025-02-15T09:12:28.801929Z",
     "iopub.status.idle": "2025-02-15T09:12:28.826591Z",
     "shell.execute_reply": "2025-02-15T09:12:28.825513Z",
     "shell.execute_reply.started": "2025-02-15T09:12:28.802287Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# # ============================\n",
    "# # 7. Fine-Tuning the Model\n",
    "# # ============================\n",
    "\n",
    "# trainer.train()\n",
    "\n",
    "# # Save the final model and tokenizer.\n",
    "# model.save_pretrained(\"llama_multi_label_final\")\n",
    "# tokenizer.save_pretrained(\"llama_multi_label_final\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-15T09:12:28.828246Z",
     "iopub.status.busy": "2025-02-15T09:12:28.827771Z",
     "iopub.status.idle": "2025-02-15T09:12:58.089846Z",
     "shell.execute_reply": "2025-02-15T09:12:58.088870Z",
     "shell.execute_reply.started": "2025-02-15T09:12:28.828173Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The cache for model files in Transformers v4.22.0 has been updated. Migrating your old cache. This is a one-time only operation. You can interrupt this and resume the migration later on by calling `transformers.utils.move_cache()`.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "24d8ad4810fe444c944d9b5111da0805",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb6172b69dcf4772a04cefd5486e8415",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6f0216106041473fbd3080b661a5c5b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "554d2998850e4684ae904ecacc73100f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import ast\n",
    "import torch\n",
    "import numpy as np\n",
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    AutoModelForSeq2SeqLM,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    DataCollatorWithPadding,\n",
    ")\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score\n",
    "\n",
    "# ================================\n",
    "# 1. Define the Label Set & Mapping\n",
    "# ================================\n",
    "strategy_labels = [\n",
    "    \"Question\",\n",
    "    \"Restatement or Paraphrasing\",\n",
    "    \"Reflection of feelings\",\n",
    "    \"Self-disclosure\",\n",
    "    \"Affirmation and Reassurance\",\n",
    "    \"Providing Suggestions\",\n",
    "    \"Information\",\n",
    "    \"Others\",\n",
    "]\n",
    "num_labels = len(strategy_labels)\n",
    "label2id = {label: i for i, label in enumerate(strategy_labels)}\n",
    "id2label = {i: label for i, label in enumerate(strategy_labels)}\n",
    "\n",
    "# ================================\n",
    "# 2. Load the CSV Datasets\n",
    "# ================================\n",
    "data_files = {\n",
    "    \"train\": \"train.csv\",\n",
    "    \"validation\": \"val.csv\",\n",
    "    \"test\": \"test.csv\"\n",
    "}\n",
    "raw_datasets = load_dataset(\"csv\", data_files=data_files)\n",
    "raw_datasets\n",
    "# ================================\n",
    "# 3. Preprocessing for Encoder Models\n",
    "# ================================\n",
    "import ast\n",
    "import re\n",
    "\n",
    "def preprocess_function(examples, tokenizer, max_length=512):\n",
    "    # Process and trim conversation texts to only include the last 6 utterances\n",
    "    new_texts = []\n",
    "    for text in examples[\"text\"]:\n",
    "        text = str(text) # Remove if error\n",
    "        # Split the conversation into utterances using a regex that looks ahead for \"usr:\" or \"sys(\"\n",
    "        turns = re.split(r'(?=usr:|sys\\()', text)\n",
    "        # Remove any empty strings and strip leading/trailing whitespace\n",
    "        turns = [turn.strip() for turn in turns if turn.strip()]\n",
    "        # Keep only the last 6 utterances if there are more than 6\n",
    "        if len(turns) > 6:\n",
    "            turns = turns[-6:]\n",
    "        # Reconstruct the text from the last utterances\n",
    "        new_texts.append(\" \".join(turns))\n",
    "    \n",
    "    # Tokenize the modified texts\n",
    "    encoding = tokenizer(\n",
    "        new_texts,\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "        max_length=max_length\n",
    "    )\n",
    "    \n",
    "    # Convert label strings (e.g. \"['Information', 'Reflection of feelings']\") into multi-hot vectors.\n",
    "    multi_hot_labels = []\n",
    "    for label_str in examples[\"label\"]:\n",
    "        try:\n",
    "            labels = ast.literal_eval(label_str)\n",
    "        except Exception:\n",
    "            labels = [label_str] if label_str else [\"Others\"]\n",
    "        multi_hot = [0.0] * num_labels\n",
    "        for lab in labels:\n",
    "            if lab in label2id:\n",
    "                multi_hot[label2id[lab]] = 1\n",
    "        multi_hot_labels.append(multi_hot)\n",
    "    \n",
    "    encoding[\"labels\"] = multi_hot_labels\n",
    "    return encoding\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-15T09:12:58.091706Z",
     "iopub.status.busy": "2025-02-15T09:12:58.090962Z",
     "iopub.status.idle": "2025-02-15T09:12:58.098622Z",
     "shell.execute_reply": "2025-02-15T09:12:58.097249Z",
     "shell.execute_reply.started": "2025-02-15T09:12:58.091655Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# ================================\n",
    "# 4. Compute Metrics Function\n",
    "# ================================\n",
    "def compute_metrics(p):\n",
    "    logits, labels = p\n",
    "    # Apply sigmoid and threshold at 0.5 to get binary predictions\n",
    "    probs = torch.sigmoid(torch.tensor(logits))\n",
    "    preds = (probs > 0.5).int().numpy()\n",
    "    f1 = f1_score(labels, preds, average=\"micro\", zero_division=0)\n",
    "    precision = precision_score(labels, preds, average=\"micro\", zero_division=0)\n",
    "    recall = recall_score(labels, preds, average=\"micro\", zero_division=0)\n",
    "    return {\"f1\": f1, \"precision\": precision, \"recall\": recall}\n",
    "\n",
    "\n",
    "# ================================\n",
    "# 5. Fine-Tuning Encoder-Based Models\n",
    "# ================================\n",
    "# Models are ordered by expected accuracy (best to lowest):\n",
    "# ALBERT > DistilBERT > MiniLM > TinyBERT\n",
    "encoder_models = [\n",
    "    # (\"microsoft/deberta-v3-large\", \"DeBERTa V3 Large\"),   # ~1.5B parameters\n",
    "    # (\"microsoft/deberta-v3-xlarge\", \"DeBERTa V3 XLarge\"),\n",
    "    (\"albert-base-v2\", \"ALBERT\"),\n",
    "    (\"distilbert-base-uncased\", \"DistilBERT\"),\n",
    "    (\"nreimers/minilm-l6-h384-uncased\", \"MiniLM\"),\n",
    "    (\"huawei-noah/TinyBERT_General_4L_312D\", \"TinyBERT\"),\n",
    "    # (\"microsoft/deberta-v3-base\", \"DeBERTa V3 Base\"),  # Stronger than BERT-based models\n",
    "    # (\"xlm-roberta-base\", \"XLM-RoBERTa Base\"),  # Multilingual alternative to RoBERTa\n",
    "    # (\"tiiuae/falcon-7b\", \"Falcon 7B\")  # Open-source model optimized for efficiency\n",
    "    # (\"NousResearch/Nous-Hermes-2-Mistral-7B\", \"Nous Hermes 7B\")\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-15T09:12:58.100264Z",
     "iopub.status.busy": "2025-02-15T09:12:58.099822Z",
     "iopub.status.idle": "2025-02-15T09:12:58.293955Z",
     "shell.execute_reply": "2025-02-15T09:12:58.292676Z",
     "shell.execute_reply.started": "2025-02-15T09:12:58.100221Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "879ff73de0b34c3a9f5ed69503421822",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/10679 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3053ebcf3fb541b0b707747bb9e81dad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/2257 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "87c1736fe8b44b7a897b3654f6f0af5a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/2389 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "raw_datasets = raw_datasets.filter(lambda example: example[\"text\"] is not None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-15T09:12:58.295371Z",
     "iopub.status.busy": "2025-02-15T09:12:58.295018Z",
     "iopub.status.idle": "2025-02-15T09:12:58.300431Z",
     "shell.execute_reply": "2025-02-15T09:12:58.299486Z",
     "shell.execute_reply.started": "2025-02-15T09:12:58.295334Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# # Training loop over your encoder models.\n",
    "# for checkpoint, model_name in encoder_models:\n",
    "#     print(f\"\\n===== Fine-tuning {model_name} ({checkpoint}) =====\")\n",
    "#     # Load tokenizer and model with a multi-label classification head.\n",
    "#     tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "#     model = AutoModelForSequenceClassification.from_pretrained(\n",
    "#         checkpoint,\n",
    "#         num_labels=num_labels,\n",
    "#         problem_type=\"multi_label_classification\",  # Uses BCEWithLogitsLoss\n",
    "#     ).to(\"cuda\")\n",
    "    \n",
    "#     # Preprocess the datasets for this tokenizer.\n",
    "#     processed_datasets = raw_datasets.map(\n",
    "#         lambda examples: preprocess_function(examples, tokenizer, max_length=512),\n",
    "#         batched=True,\n",
    "#         remove_columns=raw_datasets[\"train\"].column_names\n",
    "#     )\n",
    "#     data_collator = DataCollatorWithPadding(tokenizer)\n",
    "    \n",
    "#     # Define training arguments.\n",
    "#     training_args = TrainingArguments(\n",
    "#         output_dir=f\"./results/{model_name}\",\n",
    "#         num_train_epochs=20,\n",
    "#         per_device_train_batch_size=16,\n",
    "#         per_device_eval_batch_size=16,\n",
    "#         evaluation_strategy=\"epoch\",\n",
    "#         save_strategy=\"epoch\",\n",
    "#         load_best_model_at_end=True,\n",
    "#         metric_for_best_model=\"f1\",\n",
    "#         logging_steps=50,\n",
    "#         logging_dir=f\"./logs/{model_name}\",\n",
    "#         report_to=\"none\",\n",
    "#         fp16=True,  # Enables mixed precision training (saves memory)\n",
    "#     )\n",
    "    \n",
    "#     trainer = Trainer(\n",
    "#         model=model,\n",
    "#         args=training_args,\n",
    "#         train_dataset=processed_datasets[\"train\"],\n",
    "#         eval_dataset=processed_datasets[\"validation\"],\n",
    "#         tokenizer=tokenizer,\n",
    "#         data_collator=data_collator,\n",
    "#         compute_metrics=compute_metrics,\n",
    "#     )\n",
    "    \n",
    "#     # Fine-tune and evaluate\n",
    "#     trainer.train()\n",
    "#     results = trainer.evaluate(processed_datasets[\"test\"])\n",
    "#     print(f\"Evaluation results for {model_name}: {results}\\n\")\n",
    "    \n",
    "#     # Clean up GPU memory\n",
    "#     del model, tokenizer, trainer, processed_datasets, data_collator, training_args\n",
    "#     gc.collect()\n",
    "#     torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-15T09:12:58.301799Z",
     "iopub.status.busy": "2025-02-15T09:12:58.301498Z",
     "iopub.status.idle": "2025-02-15T09:12:58.324109Z",
     "shell.execute_reply": "2025-02-15T09:12:58.322870Z",
     "shell.execute_reply.started": "2025-02-15T09:12:58.301777Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# # ================================\n",
    "# # 6. Fine-Tuning T5 (Flan-T5) for Text-to-Text Classification\n",
    "# # ================================\n",
    "# def preprocess_function_t5(examples, tokenizer, max_input_length=512, max_target_length=32):\n",
    "#     # Add a classification prompt.\n",
    "#     inputs = [\"classify: \" + text for text in examples[\"text\"]]\n",
    "#     model_inputs = tokenizer(\n",
    "#         inputs,\n",
    "#         max_length=max_input_length,\n",
    "#         truncation=True,\n",
    "#         padding=\"max_length\"\n",
    "#     )\n",
    "#     # Convert label lists to a comma-separated string.\n",
    "#     targets = []\n",
    "#     for label_str in examples[\"label\"]:\n",
    "#         try:\n",
    "#             labels = ast.literal_eval(label_str)\n",
    "#         except Exception:\n",
    "#             labels = [label_str] if label_str else [\"Others\"]\n",
    "#         targets.append(\", \".join(labels))\n",
    "#     # Tokenize the target labels.\n",
    "#     with tokenizer.as_target_tokenizer():\n",
    "#         labels_enc = tokenizer(\n",
    "#             targets,\n",
    "#             max_length=max_target_length,\n",
    "#             truncation=True,\n",
    "#             padding=\"max_length\"\n",
    "#         )\n",
    "#     model_inputs[\"labels\"] = labels_enc[\"input_ids\"]\n",
    "#     return model_inputs\n",
    "\n",
    "\n",
    "\n",
    "# print(\"\\n===== Fine-tuning Flan-T5 (text-to-text formulation) =====\")\n",
    "# t5_checkpoint = \"google/flan-t5-small\"\n",
    "# tokenizer_t5 = AutoTokenizer.from_pretrained(t5_checkpoint)\n",
    "# model_t5 = AutoModelForSeq2SeqLM.from_pretrained(t5_checkpoint).to(\"cuda\")\n",
    "\n",
    "# processed_datasets_t5 = raw_datasets.map(\n",
    "#     lambda examples: preprocess_function_t5(examples, tokenizer_t5, max_input_length=512, max_target_length=32),\n",
    "#     batched=False\n",
    "# )\n",
    "\n",
    "# training_args_t5 = TrainingArguments(\n",
    "#     output_dir=\"./results/Flan-T5\",\n",
    "#     num_train_epochs=3,\n",
    "#     per_device_train_batch_size=16,\n",
    "#     per_device_eval_batch_size=16,\n",
    "#     evaluation_strategy=\"epoch\",\n",
    "#     save_strategy=\"epoch\",\n",
    "#     load_best_model_at_end=True,\n",
    "#     logging_steps=50,\n",
    "#     logging_dir=\"./logs/Flan-T5\",\n",
    "#     report_to=\"none\",\n",
    "# )\n",
    "\n",
    "# trainer_t5 = Trainer(\n",
    "#     model=model_t5,\n",
    "#     args=training_args_t5,\n",
    "#     train_dataset=processed_datasets_t5[\"train\"],\n",
    "#     eval_dataset=processed_datasets_t5[\"validation\"],\n",
    "#     tokenizer=tokenizer_t5,\n",
    "# )\n",
    "\n",
    "# trainer_t5.train()\n",
    "# results_t5 = trainer_t5.evaluate(processed_datasets_t5[\"test\"])\n",
    "# print(f\"Evaluation results for Flan-T5: {results_t5}\")\n",
    "\n",
    "# # Clean up T5-related GPU memory\n",
    "# del model_t5, tokenizer_t5, trainer_t5, processed_datasets_t5, training_args_t5\n",
    "# gc.collect()\n",
    "# torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-15T09:12:58.325638Z",
     "iopub.status.busy": "2025-02-15T09:12:58.325236Z",
     "iopub.status.idle": "2025-02-15T09:13:26.595792Z",
     "shell.execute_reply": "2025-02-15T09:13:26.594293Z",
     "shell.execute_reply.started": "2025-02-15T09:12:58.325601Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting flash_attn\n",
      "  Downloading flash_attn-2.7.4.post1.tar.gz (6.0 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.0/6.0 MB\u001b[0m \u001b[31m43.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from flash_attn) (2.5.1+cu121)\n",
      "Requirement already satisfied: einops in /usr/local/lib/python3.10/dist-packages (from flash_attn) (0.8.0)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->flash_attn) (3.17.0)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch->flash_attn) (4.12.2)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->flash_attn) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->flash_attn) (3.1.4)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->flash_attn) (2024.9.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch->flash_attn) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch->flash_attn) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->flash_attn) (3.0.2)\n",
      "Building wheels for collected packages: flash_attn\n",
      "  Building wheel for flash_attn (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for flash_attn: filename=flash_attn-2.7.4.post1-cp310-cp310-linux_x86_64.whl size=187797312 sha256=b267f80a08e516292cdd748056a2178a45b8abedf7fca123292eb17c21c8c87c\n",
      "  Stored in directory: /root/.cache/pip/wheels/59/ce/d5/08ea07bfc16ba218dc65a3a7ef9b6a270530bcbd2cea2ee1ca\n",
      "Successfully built flash_attn\n",
      "Installing collected packages: flash_attn\n",
      "Successfully installed flash_attn-2.7.4.post1\n"
     ]
    }
   ],
   "source": [
    "!pip install flash_attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-15T09:13:26.597721Z",
     "iopub.status.busy": "2025-02-15T09:13:26.597291Z",
     "iopub.status.idle": "2025-02-15T09:13:30.205576Z",
     "shell.execute_reply": "2025-02-15T09:13:30.204172Z",
     "shell.execute_reply.started": "2025-02-15T09:13:26.597689Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: bitsandbytes\n",
      "Version: 0.45.2\n",
      "Summary: k-bit optimizers and matrix multiplication routines.\n",
      "Home-page: https://github.com/bitsandbytes-foundation/bitsandbytes\n",
      "Author: \n",
      "Author-email: Tim Dettmers <dettmers@cs.washington.edu>\n",
      "License: MIT License\n",
      "        \n",
      "        Copyright (c) Facebook, Inc. and its affiliates.\n",
      "        \n",
      "        Permission is hereby granted, free of charge, to any person obtaining a copy\n",
      "        of this software and associated documentation files (the \"Software\"), to deal\n",
      "        in the Software without restriction, including without limitation the rights\n",
      "        to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n",
      "        copies of the Software, and to permit persons to whom the Software is\n",
      "        furnished to do so, subject to the following conditions:\n",
      "        \n",
      "        The above copyright notice and this permission notice shall be included in all\n",
      "        copies or substantial portions of the Software.\n",
      "        \n",
      "        THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n",
      "        IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n",
      "        FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n",
      "        AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n",
      "        LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n",
      "        OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n",
      "        SOFTWARE.\n",
      "        \n",
      "Location: /usr/local/lib/python3.10/dist-packages\n",
      "Requires: numpy, torch\n",
      "Required-by: \n"
     ]
    }
   ],
   "source": [
    "!pip show bitsandbytes\n",
    "# !pip install -U bitsandbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-15T09:13:30.207617Z",
     "iopub.status.busy": "2025-02-15T09:13:30.207064Z",
     "iopub.status.idle": "2025-02-15T09:13:30.212733Z",
     "shell.execute_reply": "2025-02-15T09:13:30.211594Z",
     "shell.execute_reply.started": "2025-02-15T09:13:30.207571Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# import csv\n",
    "# import ast\n",
    "# import fasttext\n",
    "\n",
    "# def convert_csv_to_fasttext_format(input_csv, output_txt):\n",
    "#     \"\"\"Converts CSV with columns 'text' and 'label' to fastText format.\"\"\"\n",
    "#     with open(input_csv, 'r', encoding='utf-8') as fin, open(output_txt, 'w', encoding='utf-8') as fout:\n",
    "#         reader = csv.DictReader(fin)\n",
    "#         for row in reader:\n",
    "#             text = row['text'].strip().replace('\\n', ' ')\n",
    "#             label_str = row['label']\n",
    "#             try:\n",
    "#                 labels = ast.literal_eval(label_str)\n",
    "#             except Exception:\n",
    "#                 labels = [label_str]\n",
    "#             # Replace spaces in labels with underscores & prefix with __label__\n",
    "#             labels = [\"__label__\" + label.strip().replace(\" \", \"_\") for label in labels]\n",
    "#             fout.write(\" \".join(labels) + \" \" + text + \"\\n\")\n",
    "\n",
    "# # Convert your CSVs (adjust paths as needed)\n",
    "# convert_csv_to_fasttext_format(\"train.csv\", \"train_ft.txt\")\n",
    "# convert_csv_to_fasttext_format(\"val.csv\", \"val_ft.txt\")\n",
    "# convert_csv_to_fasttext_format(\"test.csv\", \"test_ft.txt\")\n",
    "\n",
    "# # Train a fastText supervised classifier (using one-vs-all loss for multi-label classification).\n",
    "# model_ft = fasttext.train_supervised(\n",
    "#     input=\"train_ft.txt\",\n",
    "#     epoch=1,\n",
    "#     lr=0.1,\n",
    "#     wordNgrams=2,\n",
    "#     loss=\"ova\"  # One-vs-all loss for multi-label classification.\n",
    "# )\n",
    "\n",
    "# # Evaluate on test data.\n",
    "# result = model_ft.test(\"test_ft.txt\")\n",
    "# print(\"FastText results (N, precision, recall):\", result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-15T09:13:30.214472Z",
     "iopub.status.busy": "2025-02-15T09:13:30.214051Z",
     "iopub.status.idle": "2025-02-15T09:13:30.240470Z",
     "shell.execute_reply": "2025-02-15T09:13:30.239128Z",
     "shell.execute_reply.started": "2025-02-15T09:13:30.214432Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': 'sys(Question): How are you doing today? usr: hello I am ok how are you?',\n",
       " 'label': \"['Question']\"}"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_datasets[\"train\"][10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-15T09:13:30.242045Z",
     "iopub.status.busy": "2025-02-15T09:13:30.241660Z",
     "iopub.status.idle": "2025-02-15T09:13:30.266736Z",
     "shell.execute_reply": "2025-02-15T09:13:30.265569Z",
     "shell.execute_reply.started": "2025-02-15T09:13:30.241999Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# import torch\n",
    "# from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer, DataCollatorWithPadding\n",
    "# from datasets import load_dataset\n",
    "# import ast\n",
    "\n",
    "# # Define your label set.\n",
    "# strategy_labels = [\n",
    "#     \"Question\",\n",
    "#     \"Restatement or Paraphrasing\",\n",
    "#     \"Reflection of feelings\",\n",
    "#     \"Self-disclosure\",\n",
    "#     \"Affirmation and Reassurance\",\n",
    "#     \"Providing Suggestions\",\n",
    "#     \"Information\",\n",
    "#     \"Others\"\n",
    "# ]\n",
    "# num_labels = len(strategy_labels)\n",
    "# label2id = {label: i for i, label in enumerate(strategy_labels)}\n",
    "# id2label = {i: label for i, label in enumerate(strategy_labels)}\n",
    "\n",
    "\n",
    "# # Load your dataset (assuming CSV files for train, validation, and test).\n",
    "# raw_datasets = load_dataset(\"csv\", data_files={\"train\": \"train.csv\", \"validation\": \"val.csv\", \"test\": \"test.csv\"})\n",
    "\n",
    "# # Load tokenizer and preprocess data.\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "# processed_datasets = raw_datasets.map(\n",
    "#     lambda examples: preprocess_function(examples, tokenizer, max_length=512),\n",
    "#     batched=True,\n",
    "#     remove_columns=raw_datasets[\"train\"].column_names\n",
    "# )\n",
    "# data_collator = DataCollatorWithPadding(tokenizer)\n",
    "\n",
    "# # Load the model with a multi-label classification head.\n",
    "# model = AutoModelForSequenceClassification.from_pretrained(\n",
    "#     \"bert-base-uncased\",\n",
    "#     num_labels=num_labels,\n",
    "#     problem_type=\"multi_label_classification\"  # Uses BCEWithLogitsLoss internally.\n",
    "# )\n",
    "\n",
    "# # Define training arguments.\n",
    "# training_args = TrainingArguments(\n",
    "#     output_dir=\"./results/bert_classifier\",\n",
    "#     num_train_epochs=20,\n",
    "#     per_device_train_batch_size=16,\n",
    "#     per_device_eval_batch_size=16,\n",
    "#     evaluation_strategy=\"epoch\",\n",
    "#     save_strategy=\"epoch\",\n",
    "#     load_best_model_at_end=True,\n",
    "#     metric_for_best_model=\"f1\",\n",
    "#     logging_steps=50,\n",
    "#     logging_dir=\"./logs/bert_classifier\",\n",
    "#     report_to=\"none\",\n",
    "#     fp16=True,  # Enable mixed precision training.\n",
    "# )\n",
    "\n",
    "# # Initialize the Trainer.\n",
    "# trainer = Trainer(\n",
    "#     model=model,\n",
    "#     args=training_args,\n",
    "#     train_dataset=processed_datasets[\"train\"],\n",
    "#     eval_dataset=processed_datasets[\"validation\"],\n",
    "#     tokenizer=tokenizer,\n",
    "#     data_collator=data_collator,\n",
    "#     compute_metrics=compute_metrics,\n",
    "# )\n",
    "\n",
    "# # Train and evaluate.\n",
    "# trainer.train()\n",
    "# results = trainer.evaluate(processed_datasets[\"test\"])\n",
    "# print(\"BERT classifier results:\", results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-15T09:13:30.268426Z",
     "iopub.status.busy": "2025-02-15T09:13:30.267978Z",
     "iopub.status.idle": "2025-02-15T09:13:30.291536Z",
     "shell.execute_reply": "2025-02-15T09:13:30.290378Z",
     "shell.execute_reply.started": "2025-02-15T09:13:30.268388Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# import torch\n",
    "# import gc\n",
    "# from transformers import (\n",
    "#     AutoTokenizer,\n",
    "#     AutoModelForSequenceClassification,\n",
    "#     TrainingArguments,\n",
    "#     Trainer,\n",
    "#     DataCollatorWithPadding,\n",
    "#     BitsAndBytesConfig  # Import BitsAndBytesConfig for quantization\n",
    "# )\n",
    "# from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "\n",
    "# # Example large model checkpoint using QLoRA (adjust as needed)\n",
    "# # checkpoint = \"openlm-research/open_llama_3b_v2\" 4 hours per epoch # or \"mistralai/Mistral-7B\", etc.\n",
    "# configs=[(\"openlm-research/open_llama_3b_v2\", [\"q_proj\", \"v_proj\"]),\n",
    "#         (\"stanford-crfm/BioMedLM\", [\"c_attn\", \"c_proj\", \"c_fc\", \"c_ffn\"])]\n",
    "# # checkpoint=\"stanford-crfm/BioMedLM\" # 4 hrs ish\n",
    "# for checkpoint, target in configs:     \n",
    "#     print(f\"\\n===== Fine-tuning with QLoRA on {checkpoint} =====\")\n",
    "    \n",
    "#     # Set up BitsAndBytesConfig for 4-bit quantization.\n",
    "#     quantization_config = BitsAndBytesConfig(\n",
    "#         load_in_4bit=True,\n",
    "#         bnb_4bit_compute_dtype=torch.float16,  # or torch.bfloat16 if supported\n",
    "#         bnb_4bit_use_double_quant=True,\n",
    "#         bnb_4bit_quant_type=\"nf4\"  # \"nf4\" is common; adjust if needed\n",
    "#     )\n",
    "    \n",
    "#     print(\"Quant setup...\")\n",
    "    \n",
    "#     # Load the tokenizer\n",
    "#     tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "#     if tokenizer.pad_token is None:\n",
    "#         tokenizer.pad_token = tokenizer.eos_token\n",
    "#     # Ensure the model's config uses the same padding token\n",
    "#     # Load the model using quantization_config instead of load_in_4bit.\n",
    "#     model = AutoModelForSequenceClassification.from_pretrained(\n",
    "#         checkpoint,\n",
    "#         num_labels=num_labels,\n",
    "#         problem_type=\"multi_label_classification\",\n",
    "#         quantization_config=quantization_config,\n",
    "#         device_map=\"auto\",\n",
    "#         # attn_implementation=\"flash_attention_2\"\n",
    "#     )\n",
    "#     # model.to(\"cuda\")\n",
    "#     model.config.pad_token_id = tokenizer.pad_token_id\n",
    "    \n",
    "#     print(\"Tokenizer and Model loaded...\")\n",
    "    \n",
    "#     # Prepare the model for k-bit training (freezes most parameters and adds adapters).\n",
    "#     model.gradient_checkpointing_enable()\n",
    "#     model = prepare_model_for_kbit_training(model)\n",
    "    \n",
    "#     # Set up a LoRA configuration. Adjust target_modules based on your model architecture.\n",
    "#     lora_config = LoraConfig(\n",
    "#         r=8,                # LoRA rank\n",
    "#         lora_alpha=32,      # Scaling factor\n",
    "#         # target_modules=[\"q_proj\", \"v_proj\"],  # Typical targets; adjust if needed.\n",
    "#         # target_modules=[\"query_key_value\"],\n",
    "#         # target_modules=[\"c_attn\", \"c_proj\", \"c_fc\", \"c_ffn\"],  # Adjusted targets for GPT-2 architecture\n",
    "#         target_modules=target,\n",
    "#         lora_dropout=0.1,\n",
    "#         bias=\"none\",\n",
    "#     )\n",
    "#     model = get_peft_model(model, lora_config)\n",
    "    \n",
    "#     # LORA Setup\n",
    "    \n",
    "#     # Preprocess your datasets using your preprocess_function.\n",
    "#     processed_datasets = raw_datasets.map(\n",
    "#         lambda examples: preprocess_function(examples, tokenizer, max_length=512),\n",
    "#         batched=True,\n",
    "#         remove_columns=raw_datasets[\"train\"].column_names\n",
    "#     )\n",
    "#     data_collator = DataCollatorWithPadding(tokenizer)\n",
    "    \n",
    "#     # Define training arguments.\n",
    "#     training_args = TrainingArguments(\n",
    "#         output_dir=f\"./results/QLoRA-{checkpoint.split('/')[-1]}\",\n",
    "#         num_train_epochs=2,\n",
    "#         per_device_train_batch_size=16,  # Use a lower batch size for large models\n",
    "#         per_device_eval_batch_size=16,\n",
    "#         evaluation_strategy=\"epoch\",\n",
    "#         save_strategy=\"epoch\",\n",
    "#         load_best_model_at_end=True,\n",
    "#         metric_for_best_model=\"f1\",\n",
    "#         logging_steps=20,\n",
    "#         logging_dir=f\"./logs/QLoRA-{checkpoint.split('/')[-1]}\",\n",
    "#         report_to=\"none\",\n",
    "#         fp16=True,  # Enable mixed precision training\n",
    "#     )\n",
    "    \n",
    "#     trainer = Trainer(\n",
    "#         model=model,\n",
    "#         args=training_args,\n",
    "#         train_dataset=processed_datasets[\"train\"],\n",
    "#         eval_dataset=processed_datasets[\"validation\"],\n",
    "#         tokenizer=tokenizer,\n",
    "#         data_collator=data_collator,\n",
    "#         compute_metrics=compute_metrics,  # Your custom metric function\n",
    "#     )\n",
    "    \n",
    "#     print(\"Trainer setup... Starting Training...\")\n",
    "    \n",
    "#     # Fine-tune and evaluate.\n",
    "#     trainer.train()\n",
    "#     results = trainer.evaluate(processed_datasets[\"test\"])\n",
    "#     print(f\"Evaluation results for QLoRA on {checkpoint}: {results}\")\n",
    "    \n",
    "#     # Clean up GPU memory after training.\n",
    "#     del model, tokenizer, trainer, processed_datasets, data_collator, training_args\n",
    "#     gc.collect()\n",
    "#     torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-15T09:13:30.293338Z",
     "iopub.status.busy": "2025-02-15T09:13:30.292878Z",
     "iopub.status.idle": "2025-02-15T09:13:30.313913Z",
     "shell.execute_reply": "2025-02-15T09:13:30.312502Z",
     "shell.execute_reply.started": "2025-02-15T09:13:30.293306Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# Create a System Prompt with Chain-of-Thought Instructions\n",
    "# -----------------------------\n",
    "# Provided examples with chain-of-thought (you can modify these as needed).\n",
    "example_prompts = \"\"\"\n",
    "Example 1:\n",
    "Input: usr: I'm feeling anxious about finding work and impatient with the application process at many places I've applied.\n",
    "sys(Restatement or Paraphrasing): So from what I am understanding, you have been applying for jobs and you are having anxiety about finding one and waiting through the application process.\n",
    "usr: Yes, that's right. I keep trying and feel like I'm not getting anywhere.\n",
    "sys(Reflection of feelings): Just from the few messages we have shared I can tell how anxious this is making you.\n",
    "usr: Yes. I think that I might have to try to find a different type of work than I'm used to and I'm not sure I'll be good at trying something new.\n",
    "sys(Self-disclosure): I just recently went through a layoff at my company, and I had the same situation trying to find another job. It is very stressful.\n",
    "usr: I wish that I could get more of a reason than just a denial of my application. I'm not sure what I'm doing wrong and most companies won't give any feedback.\n",
    "sys(Providing Suggestions): What have you done to reach out for feedback from the companies? Often it helps to send emails or make phone calls to have them give you some feedback.\n",
    "usr: I haven't tried reaching out. Most of the places I've been applying are huge corporations with no one to call at HR. Maybe I should try to apply somewhere locally and see if I have better luck. At least then I could talk to a person.\n",
    "sys(Others, Providing Suggestions): I think that would be a great start! It also might help to get business cards from the people you interview with in order to be able to reach out to them in the future.\n",
    "usr: That's a great idea. I've been considering going back to school to see if I can maybe get a higher degree. I've got a Bachelor's degree now and it seems like everything takes a Master's degree these days.\n",
    "sys(Question): What type of work are you interested in?\n",
    "usr: What I'm most interested in is importing and exporting of exotic fruit but the recent restrictions on travel have really hurt the business. I've been thinking maybe I'll go back to school for something more practical like plumbing.\n",
    "sys(Information): I think that would be a great idea! It has been shown that people with higher level degrees make more money and have more opportunity than those without.\n",
    "usr: Ugh, getting accepted for a master's program sounds stressful. I'm thinking I can't go wrong by becoming a plumber or an electrician. Everyone always needs that, even with the pandemic.\n",
    "\n",
    "Reasoning: I started the conversation started with acknowledging and validating the user’s anxiety (Reflection of feelings).\n",
    "Then, I shared a personal anecdote (Self-disclosure) to build rapport and relatability.\n",
    "When the user expressed frustration about lack of feedback, I suggested an actionable step (Providing Suggestions).\n",
    "As the user explored options, additional suggestions (Providing Suggestions, Others) helped guide them.\n",
    "The user mentioned education, so I asked a question to understand their interests (Question).\n",
    "Finally, an informative response (Information) helped affirm their thoughts. I should provide more suggestions next(Providing Suggestions).\n",
    "\n",
    "Final Answer: Providing Suggestions\n",
    "\n",
    "Example 2:\n",
    "Input: usr: hi I need help. I'm under academic stress.\n",
    "sys(Question): What are you stressed about?\n",
    "usr: I am failing one of my classes. I'm worried about my scholarship.\n",
    "sys(Affirmation and Reassurance): Oh, Covid is really having a negative effect on a lot of students right now. Has school-from-home had a negative effect on you?\n",
    "usr: Yeah, I feel like I can't focus because I'm back home with my family and I just feel like there are so many distractions.\n",
    "sys(Restatement or Paraphrasing): It's interesting that you find being home more distracting than being away in a dorm.\n",
    "usr: Yeah, that's really the reason I wanted to go away from college. I don't really have my own space when I'm here. People just come in and out of my room as they please despite how busy I am.\n",
    "sys(Reflection of feelings, Self-disclosure, Question): Yeah, sometimes families can struggle with the idea of boundaries. I also went away to school for similar reasons, but I actually found college life way more distracting, to be honest. Have you told your family about the issues you're having?\n",
    "usr: I honestly am worried that they will be disappointed. Maybe that's crazy. I just remember how proud they were when I got this scholarship. I don't want them to blame me.\n",
    "\n",
    "Reasoning: I started with a (Question) because I needed to understand the user’s specific stressor.\n",
    "Once I saw that their academic struggles were tied to their home environment, I reflected on their feelings and used self-disclosure to build rapport.\n",
    "I also asked a follow-up question to help them explore a possible solution—communicating with their family.  \n",
    "I think I should ask them more (Question) and let them explore their feelings (Reflection of Feelings) so I can give better Suggestions later.\n",
    "\n",
    "Final Answer: Reflection of feelings, Question\n",
    "\n",
    "Example 3:\n",
    "Input: sys(Question): Hello! Hope you are doing well. How may I assist you?\n",
    "usr: My recent ex-girlfriend gave her daughters drugs while on a video chat with me. While being very dishonest in our relationship, I am devastated about the truth of all of it now that it's over. I really loved her and her kids; we had some great times. I turned her in for giving her kids drugs as that's not okay. Today I'm super sad. I don't have any motivation. I don't really want to be around, and I'm trapped in my thoughts with everything. I had to move to get away from her stalking me. Now I'm in a new place trying to find a job and get set up here. It's difficult to start over. It's a pretty intense situation, I admit.\n",
    "sys(Restatement or Paraphrasing): Your ex-girlfriend gave drugs to her own kids. Did I get that right?\n",
    "usr: That is what she did. Among many other things.\n",
    "sys(Affirmation and Reassurance): That is really intense and unacceptable. I feel sorry for the poor little girl and you. It looks like there is something wrong. I can understand what you are going through.\n",
    "usr: It's a difficult time. I want to do nothing. It feels very bleak right now worrying about finances and getting set up in a new state.\n",
    "sys(Reflection of feelings): It must be a hard time for you. You should not worry about her. You should think about your job and the poor girl who is trapped with her.\n",
    "usr: Her daughters now both live with their dads, so that's taken care of; the kids are safer and better off. I, however, am devastated that the whole relationship wound up a lie and the whole thing has left me in a bad place mentally.\n",
    "sys(Self-disclosure): I felt the same way when I broke up with my friend over some petty things she did. The good part is, the girls are safe now. Give some time to yourself—time will heal everything.\n",
    "usr: That's a hard light to see at the end of the tunnel when things are bleak. I'm not sure what else to do. My mind runs away a bit right now; it's just all stressful.\n",
    "\n",
    "Reasoning: The user is in a very intense situation, so I used (Restatement or Paraphrasing) to better understand their situation.\n",
    "I try to assure them that it'll be okay and that they have done the right thing(Affirmation and Reassurance).\n",
    "i think I need to make them less tense so I will use (Affirmation and Reassurance) to further help.\n",
    "\n",
    "Final Answer: Affirmation and Reassurance\n",
    "\n",
    "Example 4:\n",
    "usr: I'm feeling anxious that I am going to lose my job.\n",
    "sys(Reflection of feelings): Losing a job is always anxiety-provoking.\n",
    "usr: I hope I don't.\n",
    "sys(Question): Why do you think you will lose your job?\n",
    "usr: I am on short term disability and I am not ready to go back to work yet, and I do not have any job protection.\n",
    "sys(Restatement or Paraphrasing): Oh, so your job is not protected and your short term disability will end soon? Is that correct?\n",
    "usr: It's not ending yet, but no, my job is not protected. I live in the United States, but I have not been at my job long enough to earn protection for medical leave. You have to have been here for a year, and I started November 2020. I'm afraid that I will lose my job since I'm still on disability for the foreseeable future.\n",
    "sys(Providing Suggestions): I see. Have you spoken to HR?\n",
    "usr: I have, but they are telling me that it is up to my department manager who isn't actually getting back to me about it yet.\n",
    "sys(Restatement or Paraphrasing): Your department manager is not answering you?\n",
    "usr: No, I have sent them a few emails about it. It makes me nervous. I do not have a phone number to call and my psychiatrist really does not think I am ready to go back to the stress of my job. I wish I could just call him, but I do not have a phone number for him. Just his email.\n",
    "sys(Providing Suggestions): Have you tried mentioning that to HR?\n",
    "usr: HR is the one who gave me his email. I don't think he has a direct work line, and they're not allowed to give out personal information.\n",
    "\n",
    "Reasoning: I reflected on the user’s feelings to show empathy(Reflection of feelings), then asked a question(Question) to understand their specific concern about their job.\n",
    "Once I learned about their job protection issue, I restated it to ensure I understood correctly(Restatement or Paraphrasing).\n",
    "Since they seemed uncertain about what steps to take, I provided a (Suggestion) to help them explore their options.\n",
    "Next step I should take is to help user explore their feelings (Reflection of feelings) and ask for more information so I can help better(Information).\n",
    "\n",
    "Final Answer: Information, Reflection of feelings\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-15T09:13:30.316011Z",
     "iopub.status.busy": "2025-02-15T09:13:30.315250Z",
     "iopub.status.idle": "2025-02-15T09:13:30.336750Z",
     "shell.execute_reply": "2025-02-15T09:13:30.335578Z",
     "shell.execute_reply.started": "2025-02-15T09:13:30.315970Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# import ast\n",
    "# import random\n",
    "# import torch\n",
    "# from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, \n",
    "# from sklearn.metrics import f1_score, precision_score, recall_score\n",
    "\n",
    "# # -----------------------------\n",
    "# # Configuration and Model Loading\n",
    "# # -----------------------------\n",
    "# # Use a larger seq2seq model.\n",
    "# model_name = \"NousResearch/Nous-Hermes-2-Mistral-7B-DPO\"\n",
    "\n",
    "# # Define the list of possible strategies.\n",
    "# strategy_labels = [\n",
    "#     \"Question\",\n",
    "#     \"Restatement or Paraphrasing\",\n",
    "#     \"Reflection of feelings\",\n",
    "#     \"Self-disclosure\",\n",
    "#     \"Affirmation and Reassurance\",\n",
    "#     \"Providing Suggestions\",\n",
    "#     \"Information\",\n",
    "#     \"Others\"\n",
    "# ]\n",
    "# # Mapping for converting label to index.\n",
    "# label2id = {label: i for i, label in enumerate(strategy_labels)}\n",
    "\n",
    "# # Load tokenizer and model.\n",
    "# tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "# model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "# device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "# model.to(device)\n",
    "\n",
    "# # Create the system prompt by combining a task description with the examples.\n",
    "# def create_system_prompt_custom():\n",
    "#     task_description = (\n",
    "#         \"Your task is to analyze a conversation and predict the therapy strategy or strategies used. \"\n",
    "#         \"For each conversation, provide a detailed chain-of-thought reasoning that explains how previous parts \"\n",
    "#         \"of the conversation lead to subsequent strategies. Then, at the end, output the final predicted strategies \"\n",
    "#         \"as a comma-separated list. The possible strategies are: Question, Restatement or Paraphrasing, Reflection of feelings, \"\n",
    "#         \"Self-disclosure, Affirmation and Reassurance, Providing Suggestions, Information, and Others.\"\n",
    "#     )\n",
    "#     system_prompt = f\"{task_description}\\n\\nExamples:\\n{example_prompts}\\nBased on the above, predict the strategies for the following conversation.\"\n",
    "#     return system_prompt\n",
    "\n",
    "# system_prompt = create_system_prompt_custom()\n",
    "# print(\"=== System Prompt ===\")\n",
    "# print(system_prompt)\n",
    "# print(\"=\" * 50)\n",
    "\n",
    "# # -----------------------------\n",
    "# # Helper function to convert a list of labels into a multi-hot vector.\n",
    "# # -----------------------------\n",
    "# def convert_labels_to_vector(label_list):\n",
    "#     vector = [0] * len(strategy_labels)\n",
    "#     for label in label_list:\n",
    "#         label = label.strip()\n",
    "#         if label in label2id:\n",
    "#             vector[label2id[label]] = 1\n",
    "#     return vector\n",
    "\n",
    "# # -----------------------------\n",
    "# # Generate Prediction with Chain-of-Thought Reasoning\n",
    "# # -----------------------------\n",
    "# def generate_prediction(input_text, system_prompt, tokenizer, model, max_length=256):\n",
    "#     full_prompt = f\"{system_prompt}\\n\\nInput: {input_text}\\n\\nChain-of-Thought and Final Answer:\"\n",
    "#     inputs = tokenizer(full_prompt, return_tensors=\"pt\", truncation=True, max_length=512)\n",
    "#     inputs = {k: v.to(model.device) for k, v in inputs.items()}\n",
    "    \n",
    "#     output_ids = model.generate(\n",
    "#         **inputs,\n",
    "#         max_length=max_length,\n",
    "#         num_beams=5,\n",
    "#         early_stopping=True\n",
    "#     )\n",
    "#     prediction = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "#     return prediction.strip()\n",
    "\n",
    "# # -----------------------------\n",
    "# # Evaluate on Test Data and Compute Metrics\n",
    "# # -----------------------------\n",
    "# all_true_vectors = []\n",
    "# all_pred_vectors = []\n",
    "\n",
    "# print(\"\\n=== Testing on Test Data ===\\n\")\n",
    "# for idx, test_example in enumerate(raw_datasets[\"test\"]):\n",
    "#     test_input = test_example[\"text\"]\n",
    "#     expected_label_str = test_example[\"label\"]\n",
    "    \n",
    "#     try:\n",
    "#         true_labels = ast.literal_eval(expected_label_str)\n",
    "#     except Exception:\n",
    "#         true_labels = [expected_label_str]\n",
    "#     true_labels = [label.strip() for label in true_labels]\n",
    "    \n",
    "#     generated_output = generate_prediction(test_input, system_prompt, tokenizer, model)\n",
    "    \n",
    "#     # Attempt to extract the final answer by searching for a marker.\n",
    "#     if \"Final Answer:\" in generated_output:\n",
    "#         final_part = generated_output.split(\"Final Answer:\")[-1]\n",
    "#         pred_labels = [lab.strip() for lab in final_part.split(\",\") if lab.strip()]\n",
    "#     else:\n",
    "#         pred_labels = [lab.strip() for lab in generated_output.split(\",\") if lab.strip()]\n",
    "    \n",
    "#     true_vector = convert_labels_to_vector(true_labels)\n",
    "#     pred_vector = convert_labels_to_vector(pred_labels)\n",
    "    \n",
    "#     all_true_vectors.append(true_vector)\n",
    "#     all_pred_vectors.append(pred_vector)\n",
    "    \n",
    "#     print(f\"Example {idx+1}:\")\n",
    "#     print(\"Input:\")\n",
    "#     print(test_input)\n",
    "#     print(\"\\nChain-of-Thought and Prediction:\")\n",
    "#     print(generated_output)\n",
    "#     print(f\"\\nExpected Strategies: {true_labels}\")\n",
    "#     print(\"-\" * 50)\n",
    "\n",
    "# # Compute overall metrics.\n",
    "# f1 = f1_score(all_true_vectors, all_pred_vectors, average=\"micro\", zero_division=0)\n",
    "# precision = precision_score(all_true_vectors, all_pred_vectors, average=\"micro\", zero_division=0)\n",
    "# recall = recall_score(all_true_vectors, all_pred_vectors, average=\"micro\", zero_division=0)\n",
    "\n",
    "# print(\"\\n=== Overall Metrics ===\")\n",
    "# print(f\"F1 Score: {f1:.4f}\")\n",
    "# print(f\"Precision: {precision:.4f}\")\n",
    "# print(f\"Recall: {recall:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-15T09:13:30.338548Z",
     "iopub.status.busy": "2025-02-15T09:13:30.338147Z",
     "iopub.status.idle": "2025-02-15T09:13:36.545935Z",
     "shell.execute_reply": "2025-02-15T09:13:36.544002Z",
     "shell.execute_reply.started": "2025-02-15T09:13:30.338503Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8134b8f9995644d09735b2592c0adb8e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/1.65k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "91d09252a64447b0a166b4c18cfacd52",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.model:   0%|          | 0.00/493k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8e9bcab732ac49d89ec0cb30f71f32a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "added_tokens.json:   0%|          | 0.00/51.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c15d5baa7514ca9af4f7386b333d483",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/443 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bc517f258e7f41f0b73340c20a1cef37",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/660 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "861833d8e58b4374be9805a18645d9eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/23.9k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4387bb3a4ce34ad795d1951372bc28cb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c732707456294383abf0077a69e69c62",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00003.safetensors:   0%|          | 0.00/4.94G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-43-c670ad120f23>\u001b[0m in \u001b[0;36m<cell line: 27>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;31m# Load tokenizer and causal model.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAutoTokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m model = AutoModelForCausalLM.from_pretrained(\n\u001b[0m\u001b[1;32m     28\u001b[0m     \u001b[0mmodel_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0mtorch_dtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat16\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/auto/auto_factory.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    562\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_model_mapping\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    563\u001b[0m             \u001b[0mmodel_class\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_model_class\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_model_mapping\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 564\u001b[0;31m             return model_class.from_pretrained(\n\u001b[0m\u001b[1;32m    565\u001b[0m                 \u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mmodel_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mhub_kwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    566\u001b[0m             )\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, weights_only, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   3988\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mis_sharded\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3989\u001b[0m             \u001b[0;31m# resolved_archive_file becomes a list of files that point to the different checkpoint shards in this case.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3990\u001b[0;31m             resolved_archive_file, sharded_metadata = get_checkpoint_shard_files(\n\u001b[0m\u001b[1;32m   3991\u001b[0m                 \u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3992\u001b[0m                 \u001b[0mresolved_archive_file\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/utils/hub.py\u001b[0m in \u001b[0;36mget_checkpoint_shard_files\u001b[0;34m(pretrained_model_name_or_path, index_filename, cache_dir, force_download, proxies, resume_download, local_files_only, token, user_agent, revision, subfolder, _commit_hash, **deprecated_kwargs)\u001b[0m\n\u001b[1;32m   1096\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1097\u001b[0m             \u001b[0;31m# Load from URL\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1098\u001b[0;31m             cached_filename = cached_file(\n\u001b[0m\u001b[1;32m   1099\u001b[0m                 \u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1100\u001b[0m                 \u001b[0mshard_filename\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/utils/hub.py\u001b[0m in \u001b[0;36mcached_file\u001b[0;34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_gated_repo, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[0m\n\u001b[1;32m    401\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    402\u001b[0m         \u001b[0;31m# Load from URL or cache if already cached\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 403\u001b[0;31m         resolved_file = hf_hub_download(\n\u001b[0m\u001b[1;32m    404\u001b[0m             \u001b[0mpath_or_repo_id\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    405\u001b[0m             \u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_validators.py\u001b[0m in \u001b[0;36m_inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m             \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msmoothly_deprecate_use_auth_token\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhas_token\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhas_token\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0m_inner_fn\u001b[0m  \u001b[0;31m# type: ignore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py\u001b[0m in \u001b[0;36mhf_hub_download\u001b[0;34m(repo_id, filename, subfolder, repo_type, revision, library_name, library_version, cache_dir, local_dir, user_agent, force_download, proxies, etag_timeout, token, local_files_only, headers, endpoint, resume_download, force_filename, local_dir_use_symlinks)\u001b[0m\n\u001b[1;32m    858\u001b[0m         )\n\u001b[1;32m    859\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 860\u001b[0;31m         return _hf_hub_download_to_cache_dir(\n\u001b[0m\u001b[1;32m    861\u001b[0m             \u001b[0;31m# Destination\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    862\u001b[0m             \u001b[0mcache_dir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcache_dir\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py\u001b[0m in \u001b[0;36m_hf_hub_download_to_cache_dir\u001b[0;34m(cache_dir, repo_id, filename, repo_type, revision, endpoint, etag_timeout, headers, proxies, token, local_files_only, force_download)\u001b[0m\n\u001b[1;32m   1007\u001b[0m     \u001b[0mPath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlock_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmkdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparents\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexist_ok\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1008\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mWeakFileLock\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlock_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1009\u001b[0;31m         _download_to_tmp_and_move(\n\u001b[0m\u001b[1;32m   1010\u001b[0m             \u001b[0mincomplete_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mPath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mblob_path\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\".incomplete\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1011\u001b[0m             \u001b[0mdestination_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mPath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mblob_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py\u001b[0m in \u001b[0;36m_download_to_tmp_and_move\u001b[0;34m(incomplete_path, destination_path, url_to_download, proxies, headers, expected_size, filename, force_download)\u001b[0m\n\u001b[1;32m   1541\u001b[0m             \u001b[0m_check_disk_space\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexpected_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdestination_path\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1542\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1543\u001b[0;31m         http_get(\n\u001b[0m\u001b[1;32m   1544\u001b[0m             \u001b[0murl_to_download\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1545\u001b[0m             \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py\u001b[0m in \u001b[0;36mhttp_get\u001b[0;34m(url, temp_file, proxies, resume_size, headers, expected_size, displayed_filename, _nb_retries, _tqdm_bar)\u001b[0m\n\u001b[1;32m    450\u001b[0m         \u001b[0mnew_resume_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresume_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    451\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 452\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mchunk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miter_content\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunk_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconstants\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDOWNLOAD_CHUNK_SIZE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    453\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mchunk\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# filter out keep-alive new chunks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    454\u001b[0m                     \u001b[0mprogress\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/requests/models.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m()\u001b[0m\n\u001b[1;32m    818\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"stream\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    819\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 820\u001b[0;31m                     \u001b[0;32myield\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunk_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecode_content\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    821\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mProtocolError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    822\u001b[0m                     \u001b[0;32mraise\u001b[0m \u001b[0mChunkedEncodingError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/urllib3/response.py\u001b[0m in \u001b[0;36mstream\u001b[0;34m(self, amt, decode_content)\u001b[0m\n\u001b[1;32m   1064\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1065\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_fp_closed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fp\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_decoded_buffer\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1066\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mamt\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mamt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecode_content\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdecode_content\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1067\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1068\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/urllib3/response.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, amt, decode_content, cache_content)\u001b[0m\n\u001b[1;32m    953\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_decoded_buffer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mamt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    954\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 955\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_raw_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mamt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    956\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    957\u001b[0m         \u001b[0mflush_decoder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mamt\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mamt\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/urllib3/response.py\u001b[0m in \u001b[0;36m_raw_read\u001b[0;34m(self, amt, read1)\u001b[0m\n\u001b[1;32m    877\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    878\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_error_catcher\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 879\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fp_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mamt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mread1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mread1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mfp_closed\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34mb\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    880\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mamt\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mamt\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    881\u001b[0m                 \u001b[0;31m# Platform-specific: Buggy versions of Python.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/urllib3/response.py\u001b[0m in \u001b[0;36m_fp_read\u001b[0;34m(self, amt, read1)\u001b[0m\n\u001b[1;32m    860\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    861\u001b[0m             \u001b[0;31m# StringIO doesn't like amt=None\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 862\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mamt\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mamt\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    863\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    864\u001b[0m     def _raw_read(\n",
      "\u001b[0;32m/usr/lib/python3.10/http/client.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, amt)\u001b[0m\n\u001b[1;32m    464\u001b[0m                 \u001b[0;31m# clip the read to the \"end of response\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    465\u001b[0m                 \u001b[0mamt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlength\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 466\u001b[0;31m             \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mamt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    467\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0ms\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mamt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    468\u001b[0m                 \u001b[0;31m# Ideally, we would raise IncompleteRead if the content-length\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.10/socket.py\u001b[0m in \u001b[0;36mreadinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    703\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    704\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 705\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    706\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    707\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_timeout_occurred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.10/ssl.py\u001b[0m in \u001b[0;36mrecv_into\u001b[0;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[1;32m   1301\u001b[0m                   \u001b[0;34m\"non-zero flags not allowed in calls to recv_into() on %s\"\u001b[0m \u001b[0;34m%\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1302\u001b[0m                   self.__class__)\n\u001b[0;32m-> 1303\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnbytes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1304\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1305\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnbytes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.10/ssl.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, len, buffer)\u001b[0m\n\u001b[1;32m   1157\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1158\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mbuffer\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1159\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sslobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1160\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1161\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sslobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import ast\n",
    "import random\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score\n",
    "\n",
    "# -----------------------------\n",
    "# Configuration and Model Loading\n",
    "# -----------------------------\n",
    "# Use Mistral 7B, which is a causal LM.\n",
    "model_name = \"NousResearch/Nous-Hermes-2-Mistral-7B-DPO\"\n",
    "\n",
    "strategy_labels = [\n",
    "    \"Question\",\n",
    "    \"Restatement or Paraphrasing\",\n",
    "    \"Reflection of feelings\",\n",
    "    \"Self-disclosure\",\n",
    "    \"Affirmation and Reassurance\",\n",
    "    \"Providing Suggestions\",\n",
    "    \"Information\",\n",
    "    \"Others\"\n",
    "]\n",
    "label2id = {label: i for i, label in enumerate(strategy_labels)}\n",
    "\n",
    "# Load tokenizer and causal model.\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "# device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "# model.to(device)\n",
    "\n",
    "def create_system_prompt_custom():\n",
    "    task_description = (\n",
    "        \"Your task is to analyze a conversation and predict the therapy strategy or strategies used. \"\n",
    "        \"Before giving your final answer, explain your reasoning step-by-step, showing how earlier parts \"\n",
    "        \"of the conversation led to your prediction. The possible strategies are: Question, Restatement or Paraphrasing, \"\n",
    "        \"Reflection of feelings, Self-disclosure, Affirmation and Reassurance, Providing Suggestions, Information, and Others. \"\n",
    "        \"Finally, output the final predicted strategies as a comma-separated list.\"\n",
    "    )\n",
    "    system_prompt = f\"{task_description}\\n\\nExamples:\\n{example_prompts}\\nBased on the above, predict the strategies for the following conversation.\"\n",
    "    return system_prompt\n",
    "\n",
    "system_prompt = create_system_prompt_custom()\n",
    "print(\"=== System Prompt ===\")\n",
    "print(system_prompt)\n",
    "print(len(system_prompt))\n",
    "print(\"\\n\"+\"=\" * 50)\n",
    "\n",
    "# -----------------------------\n",
    "# Helper: Convert list of labels to a multi-hot vector.\n",
    "# -----------------------------\n",
    "def convert_labels_to_vector(label_list):\n",
    "    vector = [0] * len(strategy_labels)\n",
    "    for label in label_list:\n",
    "        label = label.strip()\n",
    "        if label in label2id:\n",
    "            vector[label2id[label]] = 1\n",
    "    return vector\n",
    "\n",
    "# -----------------------------\n",
    "# Generate Prediction with Chain-of-Thought Reasoning using a causal LM\n",
    "# -----------------------------\n",
    "def generate_prediction(input_text, system_prompt, tokenizer, model, max_new_tokens=256):\n",
    "    # Build the full prompt.\n",
    "    full_prompt = f\"{system_prompt}\\n\\nInput: {input_text}\\n\\nChain-of-Thought and Final Answer:\"\n",
    "    inputs = tokenizer(full_prompt, return_tensors=\"pt\", truncation=True, max_length=2048)\n",
    "    inputs = {k: v.to(model.device) for k, v in inputs.items()}\n",
    "    \n",
    "    output_ids = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=max_new_tokens,\n",
    "        num_beams=5,\n",
    "        early_stopping=True\n",
    "    )\n",
    "    prediction = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "    return prediction.strip()\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Evaluate on Test Data and Compute Metrics\n",
    "# -----------------------------\n",
    "all_true_vectors = []\n",
    "all_pred_vectors = []\n",
    "\n",
    "print(\"\\n=== Testing on Test Data ===\\n\")\n",
    "for idx, test_example in enumerate(raw_datasets[\"test\"]):\n",
    "    \n",
    "    if idx>=20: break\n",
    "        \n",
    "    test_input = test_example[\"text\"]\n",
    "    expected_label_str = test_example[\"label\"]\n",
    "    \n",
    "    try:\n",
    "        true_labels = ast.literal_eval(expected_label_str)\n",
    "    except Exception:\n",
    "        true_labels = [expected_label_str]\n",
    "    true_labels = [label.strip() for label in true_labels]\n",
    "    \n",
    "    generated_output = generate_prediction(test_input, system_prompt, tokenizer, model)\n",
    "    \n",
    "    # Extract final strategies from output.\n",
    "    if \"Final Answer:\" in generated_output:\n",
    "        final_part = generated_output.split(\"Final Answer:\")[-1]\n",
    "        pred_labels = [lab.strip() for lab in final_part.split(\",\") if lab.strip()]\n",
    "    else:\n",
    "        pred_labels = [lab.strip() for lab in generated_output.split(\",\") if lab.strip()]\n",
    "    \n",
    "    true_vector = convert_labels_to_vector(true_labels)\n",
    "    pred_vector = convert_labels_to_vector(pred_labels)\n",
    "    \n",
    "    all_true_vectors.append(true_vector)\n",
    "    all_pred_vectors.append(pred_vector)\n",
    "    \n",
    "    print(f\"Example {idx+1}:\")\n",
    "    print(\"Input:\")\n",
    "    print(test_input)\n",
    "    print(\"\\nChain-of-Thought and Prediction:\")\n",
    "    print(generated_output)\n",
    "    print(f\"\\nExpected Strategies: {true_labels}\\n\")\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "f1 = f1_score(all_true_vectors, all_pred_vectors, average=\"micro\", zero_division=0)\n",
    "precision = precision_score(all_true_vectors, all_pred_vectors, average=\"micro\", zero_division=0)\n",
    "recall = recall_score(all_true_vectors, all_pred_vectors, average=\"micro\", zero_division=0)\n",
    "\n",
    "print(\"\\n=== Overall Metrics ===\")\n",
    "print(f\"F1 Score: {f1:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-15T09:14:13.162620Z",
     "iopub.status.busy": "2025-02-15T09:14:13.162245Z",
     "iopub.status.idle": "2025-02-15T09:14:13.170324Z",
     "shell.execute_reply": "2025-02-15T09:14:13.169204Z",
     "shell.execute_reply.started": "2025-02-15T09:14:13.162594Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': \"sys(Self-disclosure, Affirmation and Reassurance): It's like we live the same life. I also have no car to escape! It seems so small, but it's such a huge stressor when you feel trapped in an environment you're not positive in. I feel your pain and I empathize with you completely. It'll be hard but I hope you can make it through it throughout the holidays and enjoy yourself some. Are there any pros to going back home? Any pets? usr: Thank you I appreciate that. I will be fine making it over the thanksgiving break but I am more nervous about covid-19 sending us home for good. Not many to be honest. I have a hamster but he is at school with me so nothing at home to go back to sys(Restatement or Paraphrasing, Providing Suggestions): It sounds like Covid- 19 is going to be a personal stressor for you. It's such a strange thing to have to live with already, the pandemic, and i'm sorry that it might end up pushing you where you don't want to be. Could you bring your hamster home with you? Even the smallest things could help a place feel more loving usr: Yes it is very strange and I know that it is a big stressor on all of us, i don't want to sound selfish. Yes i am bringing him home with me so that is my little piece of joy that is coming along\",\n",
       " 'label': \"['Affirmation and Reassurance', 'Providing Suggestions']\"}"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_datasets[\"test\"][5]"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [],
   "dockerImageVersionId": 30886,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
